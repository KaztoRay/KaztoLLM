import torch
from datasets import load_dataset
from transformers import (
    
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
    
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# --- ì„¤ì • ---
MODEL_ID = "mistralai/Mistral-7B-v0.3"
OUTPUT_DIR = "./kazto-security"

print("ğŸ” ì—°íƒ„ë§¥ ìì› ìµœì í™” ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘...")

# 1. 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (64GB RAMì„ ê³ ë ¤í•œ ì•ˆì •ì  ì„¤ì •)
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto", # ì—°íƒ„ë§¥ì˜ CPU/GPU ìì› ìë™ ë°°ë¶„
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)

# 3. LoRA ì„¤ì • (ë³´ì•ˆ ì§€ì‹ ì£¼ì…)
model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=16, lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 4. ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í°í™”
dataset = load_dataset("json", data_files="security_dataset_v2.jsonl", split="train")
def tokenize_func(examples):
    text = f"<s>[INST] {examples['instruction']} [/INST] {examples['response']} </s>"
    return tokenizer(text, truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_func, remove_columns=dataset.column_names)

# 5. ì—°íƒ„ë§¥ ë§ì¶¤í˜• í•™ìŠµ ì¸ì
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4, # 64GB RAMì´ë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ë„‰ë„‰íˆ ì¡ìŒ
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=1,
    save_strategy="epoch",
    report_to="none"
)

# 6. í•™ìŠµ ì‹œì‘
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("ğŸ”’ [KaztoLLM] í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()
model.save_pretrained("./final_security_adapter")
print("âœ… ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ!")