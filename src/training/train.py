import torch
import os
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType

# 1. ëª¨ë¸ ì„¤ì • (ìŠ¹ì¸ í•„ìš” ì—†ëŠ” Mistral v0.3)
MODEL_ID = "mistralai/Mistral-7B-v0.3"
DATASET_PATH = "security_dataset_v2.jsonl"
OUTPUT_DIR = "./jkazto-security-v1"

print("ğŸ” Mistral-7B ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘... (ìŠ¹ì¸ ì ˆì°¨ ì—†ìŒ)")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

# ëª¨ë¸ ë¡œë“œ (CPU ìµœì í™”)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="cpu",
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
)

# 2. Mistralìš© LoRA ì„¤ì •
# Mistralì€ q, v ì™¸ì— k, o, gate, up, down ë“± ëª¨ë“  ë ˆì´ì–´ë¥¼ í•™ìŠµí•  ë•Œ ë³´ì•ˆ ì§€ì‹ì´ ë” ì˜ ì£¼ì…ë©ë‹ˆë‹¤.
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 3. ë°ì´í„°ì…‹ ê°€ê³µ (Mistral í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©)
def formatting_mistral_func(example):
    # Mistral íŠ¹ìœ ì˜ [INST] íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì‹œì–´ ì´í–‰ ëŠ¥ë ¥ì„ ë†’ì…ë‹ˆë‹¤.
    text = (
        f"<s>[INST] ë¶„ì•¼: {example.get('domain', 'ë³´ì•ˆ ì „ë¬¸ê°€')}\n"
        f"ë¶„ì„ ìš”ì²­: {example['instruction']}\n"
        f"ë§¥ë½: {example.get('context', 'N/A')} [/INST]\n"
        f"ì „ë¬¸ê°€ ë¶„ì„: {example['response']} </s>"
    )
    return {"text": text}

dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
dataset = dataset.map(formatting_mistral_func)
tokenized_dataset = dataset.map(
    lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=1024),
    batched=True,
    remove_columns=dataset.column_names
)

# 4. ì—°íƒ„ë§¥ ìµœì í™” í•™ìŠµ ì„¤ì • (64GB RAM í™œìš©)
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=5,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_steps=1,
    save_strategy="epoch",
    use_cpu=True, # ê°•ì œ CPU ëª¨ë“œ
    report_to="none"
)

# 5. íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("ğŸ”’ [Mistral Security V1] ì—°íƒ„ë§¥ í†µí•© ë³´ì•ˆ í•™ìŠµ ì‹œì‘...")
trainer.train()

# 6. ê²°ê³¼ ì €ì¥
model.save_pretrained("./final_mistral_security_adapter")
print("âœ… í•™ìŠµ ì™„ë£Œ! './final_mistral_security_adapter'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")