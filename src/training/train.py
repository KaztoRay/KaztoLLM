import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model

# 1. ëª¨ë¸ ì„¤ì •
MODEL_ID = "meta-llama/Meta-Llama-3-8B"
DATASET_PATH = "security_dataset.jsonl"

# 2. ë¡œë“œ ë° ìµœì í™”
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

# ì—°íƒ„ë§¥ CPUë¥¼ ìœ„í•´ float32 ì‚¬ìš© (ë¹„í‘œì¤€ GPU ê°€ì† ì œì™¸)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="cpu",
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
)

# 3. LoRA ì„¤ì • (ë³´ì•ˆ ì§€ì‹ ì£¼ì…ì„ ìœ„í•œ Rank 16)
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, config)

# 4. ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

def tokenize_function(examples):
    # ë³´ì•ˆ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    texts = [f"### ë¶„ì„ ìš”ì²­: {i}\n### ë‹µë³€: {r}</s>" for i, r in zip(examples['instruction'], examples['response'])]
    return tokenizer(texts, truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)

# 5. í•™ìŠµ ì¸ì (64GB RAMì„ ì´ìš©í•œ ëŒ€ëŸ‰ ì²˜ë¦¬)
training_args = TrainingArguments(
    output_dir="./kazto-security-v3",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16, # ë©”ëª¨ë¦¬ ë¶€í•˜ë¥¼ ì¤„ì´ë©´ì„œ ëŒ€ëŸ‰ í•™ìŠµ íš¨ê³¼
    num_train_epochs=5,
    learning_rate=1e-4,
    save_strategy="epoch",
    logging_steps=5,
    use_cpu=True, # ê°•ì œ CPU ëª¨ë“œ
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("ğŸ”’ ë³´ì•ˆ ì „ë¬¸ê°€ ëª¨ë¸ í†µí•© í•™ìŠµ ì‹œì‘...")
trainer.train()
model.save_pretrained("./final_expert_adapter")