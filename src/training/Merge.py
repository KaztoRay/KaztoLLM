import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_id = "mistralai/Mistral-7B-v0.3"
adapter_path = "./final_mistral_security_adapter"
save_path = "./merged_security_mistral"

print("ğŸ”„ ëª¨ë¸ ë³‘í•© ì‹œì‘ (ì´ ì‘ì—…ì€ RAMì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤)...")
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="cpu"
)

# ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
model = PeftModel.from_pretrained(base_model, adapter_path)
merged_model = model.merge_and_unload()

# ìµœì¢… ëª¨ë¸ ì €ì¥
merged_model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"âœ… ë³‘í•© ì™„ë£Œ! '{save_path}' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")