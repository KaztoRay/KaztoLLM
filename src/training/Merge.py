# merge.py
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ê²½ë¡œ ì„¤ì • (ì‹¤ì œ í´ë”ëª…ê³¼ ì¼ì¹˜ì‹œì¼°ìŠµë‹ˆë‹¤)
base_model_path = "mistralai/Mistral-7B-v0.3"
adapter_path = "./final_security_adapter" 
save_path = "./merged_security_model"

print("ğŸ”„ ì—°íƒ„ë§¥ RAMì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë³‘í•©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="cpu"
)

# 2. ì–´ëŒ‘í„°(í•™ìŠµëœ ì§€ì‹) ë¡œë“œ ë° ë³‘í•©
if os.path.exists(os.path.join(adapter_path, "adapter_config.json")):
    model = PeftModel.from_pretrained(base_model, adapter_path)
    merged_model = model.merge_and_unload()
    
    # 3. ìµœì¢… í†µí•© ëª¨ë¸ ì €ì¥
    merged_model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"âœ… ë³‘í•© ì™„ë£Œ! '{save_path}' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
else:
    print(f"âŒ ì—ëŸ¬: '{adapter_path}' í´ë” ë‚´ì— adapter_config.jsonì´ ì—†ìŠµë‹ˆë‹¤.")
    print(f"í˜„ì¬ í´ë” ëª©ë¡: {os.listdir('.')}")